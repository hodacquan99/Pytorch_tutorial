{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4BKowSQNk-N"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR2GVbodNr2j"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # input 1 channel, output 6 channel, kernel size 3*3\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(14*14*32, 128)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        # Flatten to vector to input the neural network\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tigjCTSEOLWF",
        "outputId": "8b191e55-b283-4d04-d917-90a2a9223f1e"
      },
      "source": [
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYV12Vs8ONFB"
      },
      "source": [
        "params = list(net.parameters())"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfkSg-5AOau4",
        "outputId": "6c04f66a-36fe-4240-fc8b-b7587e5a20ec"
      },
      "source": [
        "params[0].size()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "728hNruUOict",
        "outputId": "0e7f9400-1f1a-48b8-8c28-725f9f91e2b9"
      },
      "source": [
        "params[1].size()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIZ5JXc9On98",
        "outputId": "23c550c3-c8c9-49fa-f6c4-dac2aff0a28f"
      },
      "source": [
        "input = torch.randn(1, 1, 28, 28)\n",
        "out_call = net(input)\n",
        "out_forward = net.forward(input)\n",
        "\n",
        "out_call == out_forward"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True, True, True, True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRMN_W-3Ougm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9147215-12d9-4dbb-935f-1f1d9dc9632b"
      },
      "source": [
        "net = Net()\n",
        "def print_info(self, input, output):\n",
        "    # input is a tuple of packed inputs\n",
        "    # output is a Tensor. output.data is the Tensor we are interested\n",
        "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
        "    \n",
        "    print('')\n",
        "    print('input: ', type(input), ', len: ', len(input))\n",
        "    print('input[0]: ', type(input[0]), ', shape: ', input[0].shape)\n",
        "    print('output: ', type(output), ', len: ', len(output), output.data.shape)\n",
        "    \n",
        "\n",
        "net.conv2.register_forward_hook(print_info)\n",
        "\n",
        "out = net(input)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inside Conv2d forward\n",
            "\n",
            "input:  <class 'tuple'> , len:  1\n",
            "input[0]:  <class 'torch.Tensor'> , shape:  torch.Size([1, 32, 28, 28])\n",
            "output:  <class 'torch.Tensor'> , len:  1 torch.Size([1, 32, 28, 28])\n",
            "torch.Size([1, 32, 14, 14])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ6j_pwR1VaA",
        "outputId": "516e2415-c0f3-49b1-be94-c1b61d1d2e64"
      },
      "source": [
        "\n",
        "target = torch.tensor([3])\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "net = Net()\n",
        "def print_backward_info(self, grad_input, grad_output):\n",
        "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
        "    print('grad_input: ', type(grad_input), ', len: ', len(grad_input))\n",
        "    print('grad_output: ', type(grad_output), ', len: ', len(grad_output))\n",
        "    print('grad_output[0]: ', type(grad_output[0]), ', size: ', grad_output[0].shape)\n",
        "\n",
        "\n",
        "net.conv1.register_backward_hook(print_backward_info)\n",
        "\n",
        "out = net(input)\n",
        "err = loss_fn(out, target)\n",
        "err.backward()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inside Conv2d backward\n",
            "grad_input:  <class 'tuple'> , len:  3\n",
            "grad_output:  <class 'tuple'> , len:  1\n",
            "grad_output[0]:  <class 'torch.Tensor'> , size:  torch.Size([1, 32, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HnLWCs5AFju"
      },
      "source": [
        "def __call__(self, *input, **kwargs):\n",
        "  for hook in self._forward_pre_hooks.values():\n",
        "    hook(self, input)\n",
        "  \n",
        "  result = self.forward(*input, **kwargs)\n",
        "  \n",
        "  for hook in self._forward_hooks.values():\n",
        "    hook(self, input, result)\n",
        "    # TODO\n",
        "  \n",
        "  for hook in self._backward_hooks.values():\n",
        "    # TODO\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pyk6PPUZIBR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}